{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QsRcv21vBOZv",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3d572334-76b6-4f6b-84cd-3259d3822068"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: diffusers in /usr/local/lib/python3.12/dist-packages (0.35.1)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.12/dist-packages (4.55.4)\n",
            "Requirement already satisfied: accelerate in /usr/local/lib/python3.12/dist-packages (1.10.1)\n",
            "Requirement already satisfied: safetensors in /usr/local/lib/python3.12/dist-packages (0.6.2)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.12/dist-packages (2.8.0+cu126)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.12/dist-packages (0.23.0+cu126)\n",
            "Requirement already satisfied: gradio in /usr/local/lib/python3.12/dist-packages (5.43.1)\n",
            "Requirement already satisfied: importlib_metadata in /usr/local/lib/python3.12/dist-packages (from diffusers) (8.7.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from diffusers) (3.19.1)\n",
            "Requirement already satisfied: huggingface-hub>=0.34.0 in /usr/local/lib/python3.12/dist-packages (from diffusers) (0.34.4)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from diffusers) (2.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.12/dist-packages (from diffusers) (2024.11.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from diffusers) (2.32.4)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.12/dist-packages (from diffusers) (11.3.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (25.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.12/dist-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.21.4)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.12/dist-packages (from transformers) (4.67.1)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.12/dist-packages (from accelerate) (5.9.5)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.12/dist-packages (from torch) (4.15.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch) (75.2.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch) (1.13.3)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from torch) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.12/dist-packages (from torch) (2025.3.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.80)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch) (11.3.0.4)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch) (10.3.7.77)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch) (11.7.1.2)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch) (12.5.4.2)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.3 in /usr/local/lib/python3.12/dist-packages (from torch) (2.27.3)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.85)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch) (1.11.1.6)\n",
            "Requirement already satisfied: triton==3.4.0 in /usr/local/lib/python3.12/dist-packages (from torch) (3.4.0)\n",
            "Requirement already satisfied: aiofiles<25.0,>=22.0 in /usr/local/lib/python3.12/dist-packages (from gradio) (24.1.0)\n",
            "Requirement already satisfied: anyio<5.0,>=3.0 in /usr/local/lib/python3.12/dist-packages (from gradio) (4.10.0)\n",
            "Requirement already satisfied: brotli>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from gradio) (1.1.0)\n",
            "Requirement already satisfied: fastapi<1.0,>=0.115.2 in /usr/local/lib/python3.12/dist-packages (from gradio) (0.116.1)\n",
            "Requirement already satisfied: ffmpy in /usr/local/lib/python3.12/dist-packages (from gradio) (0.6.1)\n",
            "Requirement already satisfied: gradio-client==1.12.1 in /usr/local/lib/python3.12/dist-packages (from gradio) (1.12.1)\n",
            "Requirement already satisfied: groovy~=0.1 in /usr/local/lib/python3.12/dist-packages (from gradio) (0.1.2)\n",
            "Requirement already satisfied: httpx<1.0,>=0.24.1 in /usr/local/lib/python3.12/dist-packages (from gradio) (0.28.1)\n",
            "Requirement already satisfied: markupsafe<4.0,>=2.0 in /usr/local/lib/python3.12/dist-packages (from gradio) (3.0.2)\n",
            "Requirement already satisfied: orjson~=3.0 in /usr/local/lib/python3.12/dist-packages (from gradio) (3.11.2)\n",
            "Requirement already satisfied: pandas<3.0,>=1.0 in /usr/local/lib/python3.12/dist-packages (from gradio) (2.2.2)\n",
            "Requirement already satisfied: pydantic<2.12,>=2.0 in /usr/local/lib/python3.12/dist-packages (from gradio) (2.11.7)\n",
            "Requirement already satisfied: pydub in /usr/local/lib/python3.12/dist-packages (from gradio) (0.25.1)\n",
            "Requirement already satisfied: python-multipart>=0.0.18 in /usr/local/lib/python3.12/dist-packages (from gradio) (0.0.20)\n",
            "Requirement already satisfied: ruff>=0.9.3 in /usr/local/lib/python3.12/dist-packages (from gradio) (0.12.10)\n",
            "Requirement already satisfied: safehttpx<0.2.0,>=0.1.6 in /usr/local/lib/python3.12/dist-packages (from gradio) (0.1.6)\n",
            "Requirement already satisfied: semantic-version~=2.0 in /usr/local/lib/python3.12/dist-packages (from gradio) (2.10.0)\n",
            "Requirement already satisfied: starlette<1.0,>=0.40.0 in /usr/local/lib/python3.12/dist-packages (from gradio) (0.47.3)\n",
            "Requirement already satisfied: tomlkit<0.14.0,>=0.12.0 in /usr/local/lib/python3.12/dist-packages (from gradio) (0.13.3)\n",
            "Requirement already satisfied: typer<1.0,>=0.12 in /usr/local/lib/python3.12/dist-packages (from gradio) (0.16.1)\n",
            "Requirement already satisfied: uvicorn>=0.14.0 in /usr/local/lib/python3.12/dist-packages (from gradio) (0.35.0)\n",
            "Requirement already satisfied: websockets<16.0,>=10.0 in /usr/local/lib/python3.12/dist-packages (from gradio-client==1.12.1->gradio) (15.0.1)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.12/dist-packages (from anyio<5.0,>=3.0->gradio) (3.10)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.12/dist-packages (from anyio<5.0,>=3.0->gradio) (1.3.1)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.12/dist-packages (from httpx<1.0,>=0.24.1->gradio) (2025.8.3)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx<1.0,>=0.24.1->gradio) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.12/dist-packages (from httpcore==1.*->httpx<1.0,>=0.24.1->gradio) (0.16.0)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.34.0->diffusers) (1.1.8)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas<3.0,>=1.0->gradio) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas<3.0,>=1.0->gradio) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas<3.0,>=1.0->gradio) (2025.2)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<2.12,>=2.0->gradio) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.12/dist-packages (from pydantic<2.12,>=2.0->gradio) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<2.12,>=2.0->gradio) (0.4.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
            "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.12/dist-packages (from typer<1.0,>=0.12->gradio) (8.2.1)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.12/dist-packages (from typer<1.0,>=0.12->gradio) (1.5.4)\n",
            "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.12/dist-packages (from typer<1.0,>=0.12->gradio) (13.9.4)\n",
            "Requirement already satisfied: zipp>=3.20 in /usr/local/lib/python3.12/dist-packages (from importlib_metadata->diffusers) (3.23.0)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->diffusers) (3.4.3)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->diffusers) (2.5.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->pandas<3.0,>=1.0->gradio) (1.17.0)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.12/dist-packages (from rich>=10.11.0->typer<1.0,>=0.12->gradio) (4.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.12/dist-packages (from rich>=10.11.0->typer<1.0,>=0.12->gradio) (2.19.2)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.12/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0,>=0.12->gradio) (0.1.2)\n"
          ]
        }
      ],
      "source": [
        "!pip install diffusers transformers accelerate safetensors torch torchvision gradio"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from PIL import Image\n",
        "import torchvision.transforms as transforms\n",
        "import torchvision.models as models\n",
        "import copy\n",
        "\n",
        "# Load and preprocess image\n",
        "def image_loader(image_path, imsize=512):\n",
        "    loader = transforms.Compose([\n",
        "        transforms.Resize((imsize, imsize)),\n",
        "        transforms.ToTensor()\n",
        "    ])\n",
        "    image = Image.open(image_path)\n",
        "    image = loader(image).unsqueeze(0)\n",
        "    return image.to(torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\"), torch.float)\n",
        "\n",
        "# Model\n",
        "cnn = models.vgg19(weights=models.VGG19_Weights.DEFAULT).features.eval().to(\"cuda\")\n",
        "\n",
        "# Style transfer function (simplified)\n",
        "def run_style_transfer(cnn, content_img, style_img, num_steps=200):\n",
        "    input_img = content_img.clone()\n",
        "    optimizer = optim.LBFGS([input_img.requires_grad_()])\n",
        "    mse_loss = nn.MSELoss()\n",
        "\n",
        "    style_weight = 1e6\n",
        "    content_weight = 1\n",
        "\n",
        "    for step in range(num_steps):\n",
        "        def closure():\n",
        "            optimizer.zero_grad()\n",
        "            # simple content loss (for demo, you can expand later)\n",
        "            loss = mse_loss(input_img, content_img) + style_weight * mse_loss(input_img, style_img)\n",
        "            loss.backward()\n",
        "            return loss\n",
        "        optimizer.step(closure)\n",
        "\n",
        "    return input_img.detach()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 444
        },
        "id": "pdbckjZHHPIW",
        "outputId": "18fc58fa-b8d4-426f-951a-82e848198a65"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading: \"https://download.pytorch.org/models/vgg19-dcbb9e9d.pth\" to /root/.cache/torch/hub/checkpoints/vgg19-dcbb9e9d.pth\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 548M/548M [00:07<00:00, 80.2MB/s]\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "Found no NVIDIA driver on your system. Please check that you have an NVIDIA GPU and installed a driver from http://www.nvidia.com/Download/index.aspx",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-3823537166.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;31m# Model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m \u001b[0mcnn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvgg19\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweights\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mVGG19_Weights\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDEFAULT\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"cuda\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;31m# Style transfer function (simplified)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36mto\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1367\u001b[0m                     \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1368\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1369\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconvert\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1370\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1371\u001b[0m     def register_full_backward_pre_hook(\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    926\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrecurse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    927\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchildren\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 928\u001b[0;31m                 \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    929\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    930\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mcompute_should_use_set_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensor_applied\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    953\u001b[0m             \u001b[0;31m# `with torch.no_grad():`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    954\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 955\u001b[0;31m                 \u001b[0mparam_applied\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparam\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    956\u001b[0m             \u001b[0mp_should_use_set_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompute_should_use_set_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparam\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparam_applied\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    957\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36mconvert\u001b[0;34m(t)\u001b[0m\n\u001b[1;32m   1353\u001b[0m                         \u001b[0mmemory_format\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconvert_to_format\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1354\u001b[0m                     )\n\u001b[0;32m-> 1355\u001b[0;31m                 return t.to(\n\u001b[0m\u001b[1;32m   1356\u001b[0m                     \u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1357\u001b[0m                     \u001b[0mdtype\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_floating_point\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_complex\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py\u001b[0m in \u001b[0;36m_lazy_init\u001b[0;34m()\u001b[0m\n\u001b[1;32m    410\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;34m\"CUDA_MODULE_LOADING\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menviron\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    411\u001b[0m             \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menviron\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"CUDA_MODULE_LOADING\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"LAZY\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 412\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_cuda_init\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    413\u001b[0m         \u001b[0;31m# Some of the queued calls may reentrantly call _lazy_init();\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    414\u001b[0m         \u001b[0;31m# we need to just return without initializing in that case.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: Found no NVIDIA driver on your system. Please check that you have an NVIDIA GPU and installed a driver from http://www.nvidia.com/Download/index.aspx"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from diffusers import StableDiffusionPipeline\n",
        "\n",
        "pipe = StableDiffusionPipeline.from_pretrained(\n",
        "    \"runwayml/stable-diffusion-v1-5\",\n",
        "    torch_dtype=torch.float16\n",
        ")\n",
        "pipe = pipe.to(\"cuda\")\n",
        "\n",
        "def generate_image(prompt):\n",
        "    image = pipe(prompt).images[0]\n",
        "    return image\n"
      ],
      "metadata": {
        "id": "uQysJhCsH9Z6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import gradio as gr\n",
        "\n",
        "def style_transfer_ui(content, style):\n",
        "    content_img = image_loader(content.name)\n",
        "    style_img = image_loader(style.name)\n",
        "    output = run_style_transfer(cnn, content_img, style_img)\n",
        "    return transforms.ToPILImage()(output.squeeze(0))\n",
        "\n",
        "def text2img_ui(prompt):\n",
        "    return generate_image(prompt)\n",
        "\n",
        "with gr.Blocks() as demo:\n",
        "    gr.Markdown(\"# 🎨 AI Art Generator\")\n",
        "\n",
        "    with gr.Tab(\"Text-to-Image (Stable Diffusion)\"):\n",
        "        prompt_input = gr.Textbox(label=\"Enter Prompt\")\n",
        "        output_img2 = gr.Image()\n",
        "        btn2 = gr.Button(\"Generate AI Image\")\n",
        "        btn2.click(fn=text2img_ui, inputs=prompt_input, outputs=output_img2)\n",
        "\n",
        "demo.launch(share=True)"
      ],
      "metadata": {
        "id": "11vbp9pIIPGG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torchvision.models as models\n",
        "import torchvision.transforms as transforms\n",
        "from PIL import Image\n",
        "import gradio as gr\n",
        "\n",
        "# ===================== #\n",
        "# Image Preprocessing\n",
        "# ===================== #\n",
        "imsize = 512 if torch.cuda.is_available() else 256\n",
        "\n",
        "loader = transforms.Compose([\n",
        "    transforms.Resize((imsize, imsize)),\n",
        "    transforms.ToTensor()\n",
        "])\n",
        "\n",
        "def image_loader(image_path):\n",
        "    image = Image.open(image_path).convert(\"RGB\")\n",
        "    image = loader(image).unsqueeze(0)  # [1,3,H,W]\n",
        "    return image.to(device, torch.float)\n",
        "\n",
        "unloader = transforms.ToPILImage()\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# ===================== #\n",
        "# Load VGG19 Model\n",
        "# ===================== #\n",
        "cnn = models.vgg19(weights=models.VGG19_Weights.DEFAULT).features.to(device).eval()\n",
        "\n",
        "# Content & Style Layers\n",
        "content_layers = ['conv_4']\n",
        "style_layers = ['conv_1', 'conv_2', 'conv_3', 'conv_4', 'conv_5']\n",
        "\n",
        "# ===================== #\n",
        "# Loss Functions\n",
        "# ===================== #\n",
        "class ContentLoss(nn.Module):\n",
        "    def __init__(self, target):\n",
        "        super(ContentLoss, self).__init__()\n",
        "        self.target = target.detach()\n",
        "\n",
        "    def forward(self, x):\n",
        "        self.loss = nn.functional.mse_loss(x, self.target)\n",
        "        return x\n",
        "\n",
        "def gram_matrix(x):\n",
        "    a, b, c, d = x.size()\n",
        "    features = x.view(a * b, c * d)\n",
        "    G = torch.mm(features, features.t())\n",
        "    return G.div(a * b * c * d)\n",
        "\n",
        "class StyleLoss(nn.Module):\n",
        "    def __init__(self, target_feature):\n",
        "        super(StyleLoss, self).__init__()\n",
        "        self.target = gram_matrix(target_feature).detach()\n",
        "\n",
        "    def forward(self, x):\n",
        "        G = gram_matrix(x)\n",
        "        self.loss = nn.functional.mse_loss(G, self.target)\n",
        "        return x\n",
        "\n",
        "# ===================== #\n",
        "# Style Transfer Function\n",
        "# ===================== #\n",
        "def run_style_transfer(content_path, style_path, num_steps=200,\n",
        "                       style_weight=1e6, content_weight=1):\n",
        "    # Load images\n",
        "    content_img = image_loader(content_path)\n",
        "    style_img = image_loader(style_path)\n",
        "\n",
        "    assert content_img.size() == style_img.size(), \\\n",
        "        \"Content and Style images must be the same size\"\n",
        "\n",
        "    input_img = content_img.clone()\n",
        "\n",
        "    # Build model\n",
        "    cnn_normalization_mean = torch.tensor([0.485, 0.456, 0.406]).to(device)\n",
        "    cnn_normalization_std = torch.tensor([0.229, 0.224, 0.225]).to(device)\n",
        "\n",
        "    normalization = transforms.Normalize(mean=cnn_normalization_mean,\n",
        "                                         std=cnn_normalization_std)\n",
        "\n",
        "    content_losses = []\n",
        "    style_losses = []\n",
        "\n",
        "    model = nn.Sequential(normalization)\n",
        "\n",
        "    i = 0\n",
        "    for layer in cnn.children():\n",
        "        if isinstance(layer, nn.Conv2d):\n",
        "            i += 1\n",
        "            name = f'conv_{i}'\n",
        "        elif isinstance(layer, nn.ReLU):\n",
        "            name = f'relu_{i}'\n",
        "            layer = nn.ReLU(inplace=False)\n",
        "        elif isinstance(layer, nn.MaxPool2d):\n",
        "            name = f'pool_{i}'\n",
        "        elif isinstance(layer, nn.BatchNorm2d):\n",
        "            name = f'bn_{i}'\n",
        "        else:\n",
        "            raise RuntimeError(f\"Unrecognized layer: {layer.__class__.__name__}\")\n",
        "\n",
        "        model.add_module(name, layer)\n",
        "\n",
        "        if name in content_layers:\n",
        "            target = model(content_img).detach()\n",
        "            content_loss = ContentLoss(target)\n",
        "            model.add_module(f\"content_loss_{i}\", content_loss)\n",
        "            content_losses.append(content_loss)\n",
        "\n",
        "        if name in style_layers:\n",
        "            target_feature = model(style_img).detach()\n",
        "            style_loss = StyleLoss(target_feature)\n",
        "            model.add_module(f\"style_loss_{i}\", style_loss)\n",
        "            style_losses.append(style_loss)\n",
        "\n",
        "    # Trim model after last loss\n",
        "    for i in range(len(model) - 1, -1, -1):\n",
        "        if isinstance(model[i], ContentLoss) or isinstance(model[i], StyleLoss):\n",
        "            break\n",
        "    model = model[:i+1]\n",
        "\n",
        "    optimizer = optim.LBFGS([input_img.requires_grad_()])\n",
        "\n",
        "    run = [0]\n",
        "    while run[0] <= num_steps:\n",
        "        def closure():\n",
        "            input_img.data.clamp_(0, 1)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            model(input_img)\n",
        "            style_score = sum([sl.loss for sl in style_losses])\n",
        "            content_score = sum([cl.loss for cl in content_losses])\n",
        "\n",
        "            loss = style_score * style_weight + content_score * content_weight\n",
        "            loss.backward()\n",
        "\n",
        "            run[0] += 1\n",
        "            return loss\n",
        "        optimizer.step(closure)\n",
        "\n",
        "    input_img.data.clamp_(0, 1)\n",
        "\n",
        "    # Convert tensor to image\n",
        "    output = input_img.cpu().clone()\n",
        "    output = output.squeeze(0)\n",
        "    image = unloader(output)\n",
        "    return image\n",
        "\n",
        "# ===================== #\n",
        "# Gradio UI\n",
        "# ===================== #\n",
        "def transfer(content, style):\n",
        "    output = run_style_transfer(content, style, num_steps=200)\n",
        "    return output\n",
        "\n",
        "with gr.Blocks() as demo:\n",
        "    gr.Markdown(\"🎨 **Neural Style Transfer**\")\n",
        "    with gr.Row():\n",
        "        content = gr.Image(type=\"filepath\", label=\"Upload Content Image\")\n",
        "        style = gr.Image(type=\"filepath\", label=\"Upload Style Image\")\n",
        "    output = gr.Image(label=\"Styled Output\")\n",
        "\n",
        "    btn = gr.Button(\"Transfer Style\")\n",
        "    btn.click(transfer, inputs=[content, style], outputs=[output])\n",
        "\n",
        "demo.launch()\n"
      ],
      "metadata": {
        "id": "bteauFa2e680"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ==========================================\n",
        "# Final Year Major Project – BSC.IT\n",
        "# AI Prompt Enhancer + Stable Diffusion + Improved Neural Style Transfer\n",
        "# ==========================================\n",
        "\n",
        "import requests\n",
        "import torch\n",
        "from io import BytesIO\n",
        "from PIL import Image\n",
        "from diffusers import DiffusionPipeline\n",
        "from torchvision import transforms, models\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import os\n",
        "import numpy as np\n",
        "\n",
        "# -------------------------\n",
        "# CONFIGURATION\n",
        "# -------------------------\n",
        "GEMINI_API_KEY = os.getenv(\"GEMINI_API_KEY\", \"AIzaSyC38sUGvasn8QDBoMAvzkHt7H0jrRdCoGo\")  # Use env variable\n",
        "HF_MODEL = \"runwayml/stable-diffusion-v1-5\"\n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "print(f\"✅ Using device: {device}\")\n",
        "\n",
        "# -------------------------\n",
        "# 1. AI Prompt Enhancer (Gemini API)\n",
        "# -------------------------\n",
        "def enhance_prompt(prompt: str) -> str:\n",
        "    \"\"\"Enhance user prompt using Gemini API for better Stable Diffusion results\"\"\"\n",
        "    url = \"https://generativelanguage.googleapis.com/v1beta/models/gemini-2.0-flash:generateContent\"\n",
        "    headers = {\"Content-Type\": \"application/json\"}\n",
        "    payload = {\n",
        "        \"contents\": [{\"parts\": [{\"text\": f\"Enhance this prompt for Stable Diffusion art generation. Make it more detailed and artistic but keep it under 70 words. Return only the enhanced prompt, no explanations:\\nOriginal prompt: {prompt}\"}]}]\n",
        "    }\n",
        "\n",
        "    try:\n",
        "        response = requests.post(f\"{url}?key={GEMINI_API_KEY}\", headers=headers, json=payload, timeout=10)\n",
        "        data = response.json()\n",
        "\n",
        "        enhanced = data[\"candidates\"][0][\"content\"][\"parts\"][0][\"text\"]\n",
        "        # Clean out markdown formatting or explanations\n",
        "        enhanced = enhanced.strip().split(\"```\")[0].strip('\"').strip()\n",
        "\n",
        "        # Additional check: if still too long, truncate intelligently\n",
        "        words = enhanced.split()\n",
        "        if len(words) > 70:\n",
        "            enhanced = ' '.join(words[:70])\n",
        "\n",
        "        print(f\"✨ Prompt enhanced successfully!\")\n",
        "        return enhanced\n",
        "    except Exception as e:\n",
        "        print(f\"⚠️ Gemini enhancement failed: {e}. Using original prompt.\")\n",
        "        return prompt\n",
        "\n",
        "# -------------------------\n",
        "# 2. AI Image Generator (Stable Diffusion)\n",
        "# -------------------------\n",
        "def generate_image(prompt: str, filename=\"generated.png\"):\n",
        "    \"\"\"Generate image using Stable Diffusion\"\"\"\n",
        "    print(\"🎨 Generating image with Stable Diffusion...\")\n",
        "    try:\n",
        "        pipe = DiffusionPipeline.from_pretrained(HF_MODEL, torch_dtype=torch.float16)\n",
        "        pipe = pipe.to(device)\n",
        "\n",
        "        # Generate image with improved settings\n",
        "        image = pipe(\n",
        "            prompt,\n",
        "            num_inference_steps=50,\n",
        "            guidance_scale=7.5,\n",
        "            width=512,\n",
        "            height=512\n",
        "        ).images[0]\n",
        "\n",
        "        image.save(filename)\n",
        "        print(f\"✅ Image saved as {filename}\")\n",
        "        return image\n",
        "    except Exception as e:\n",
        "        print(f\"❌ Image generation failed: {e}\")\n",
        "        return None\n",
        "\n",
        "# -------------------------\n",
        "# 3. Improved Neural Style Transfer\n",
        "# -------------------------\n",
        "\n",
        "class ContentLoss(nn.Module):\n",
        "    \"\"\"Content loss for preserving image content\"\"\"\n",
        "    def __init__(self):\n",
        "        super(ContentLoss, self).__init__()\n",
        "        self.target = None\n",
        "        self.loss = None\n",
        "\n",
        "    def set_target(self, target):\n",
        "        self.target = target.detach()\n",
        "\n",
        "    def forward(self, x):\n",
        "        if self.target is not None:\n",
        "            self.loss = nn.functional.mse_loss(x, self.target)\n",
        "        return x\n",
        "\n",
        "class StyleLoss(nn.Module):\n",
        "    \"\"\"Style loss using Gram matrices\"\"\"\n",
        "    def __init__(self):\n",
        "        super(StyleLoss, self).__init__()\n",
        "        self.target = None\n",
        "        self.loss = None\n",
        "\n",
        "    def gram_matrix(self, tensor):\n",
        "        b, c, h, w = tensor.size()\n",
        "        features = tensor.view(b * c, h * w)\n",
        "        G = torch.mm(features, features.t())\n",
        "        return G.div(b * c * h * w)\n",
        "\n",
        "    def set_target(self, target):\n",
        "        self.target = self.gram_matrix(target).detach()\n",
        "\n",
        "    def forward(self, x):\n",
        "        if self.target is not None:\n",
        "            G = self.gram_matrix(x)\n",
        "            self.loss = nn.functional.mse_loss(G, self.target)\n",
        "        return x\n",
        "\n",
        "class ImprovedStyleTransfer:\n",
        "    \"\"\"Improved Neural Style Transfer using VGG19\"\"\"\n",
        "    def __init__(self, device):\n",
        "        self.device = device\n",
        "\n",
        "        # Load pre-trained VGG19\n",
        "        vgg = models.vgg19(weights=models.VGG19_Weights.DEFAULT).features.to(device).eval()\n",
        "\n",
        "        # Normalization for VGG19\n",
        "        self.normalization = transforms.Normalize(\n",
        "            mean=[0.485, 0.456, 0.406],\n",
        "            std=[0.229, 0.224, 0.225]\n",
        "        ).to(device)\n",
        "\n",
        "        # Define which layers to use for content and style\n",
        "        self.content_layers = ['conv_4']\n",
        "        self.style_layers = ['conv_1', 'conv_2', 'conv_3', 'conv_4', 'conv_5']\n",
        "\n",
        "        self.model, self.style_losses, self.content_losses = self.build_model(vgg)\n",
        "\n",
        "    def build_model(self, cnn):\n",
        "        \"\"\"Build the neural network with loss functions\"\"\"\n",
        "        content_losses = []\n",
        "        style_losses = []\n",
        "\n",
        "        model = nn.Sequential(self.normalization)\n",
        "\n",
        "        i = 0\n",
        "        for layer in cnn.children():\n",
        "            if isinstance(layer, nn.Conv2d):\n",
        "                i += 1\n",
        "                name = f'conv_{i}'\n",
        "            elif isinstance(layer, nn.ReLU):\n",
        "                name = f'relu_{i}'\n",
        "                layer = nn.ReLU(inplace=False)  # Important: don't use inplace\n",
        "            elif isinstance(layer, nn.MaxPool2d):\n",
        "                name = f'pool_{i}'\n",
        "            elif isinstance(layer, nn.BatchNorm2d):\n",
        "                name = f'bn_{i}'\n",
        "            else:\n",
        "                raise RuntimeError(f'Unrecognized layer: {layer.__class__.__name__}')\n",
        "\n",
        "            model.add_module(name, layer)\n",
        "\n",
        "            if name in self.content_layers:\n",
        "                content_loss = ContentLoss()\n",
        "                model.add_module(f\"content_loss_{i}\", content_loss)\n",
        "                content_losses.append(content_loss)\n",
        "\n",
        "            if name in self.style_layers:\n",
        "                style_loss = StyleLoss()\n",
        "                model.add_module(f\"style_loss_{i}\", style_loss)\n",
        "                style_losses.append(style_loss)\n",
        "\n",
        "        # Trim model after last loss layer\n",
        "        for i in range(len(model) - 1, -1, -1):\n",
        "            if isinstance(model[i], (ContentLoss, StyleLoss)):\n",
        "                break\n",
        "\n",
        "        model = model[:(i + 1)]\n",
        "        return model, style_losses, content_losses\n",
        "\n",
        "    def image_loader(self, image, imsize=512):\n",
        "        \"\"\"Load and preprocess image\"\"\"\n",
        "        if isinstance(image, str):\n",
        "            image = Image.open(image)\n",
        "\n",
        "        if image.mode != 'RGB':\n",
        "            image = image.convert('RGB')\n",
        "\n",
        "        loader = transforms.Compose([\n",
        "            transforms.Resize((imsize, imsize)),\n",
        "            transforms.ToTensor()\n",
        "        ])\n",
        "\n",
        "        image = loader(image).unsqueeze(0)\n",
        "        return image.to(self.device, torch.float)\n",
        "\n",
        "    def run_style_transfer(self, content_img, style_img, num_steps=150,\n",
        "                          style_weight=100, content_weight=1, lr=0.01):\n",
        "        \"\"\"Main style transfer function with optimized parameters\"\"\"\n",
        "\n",
        "        print(f\"🎭 Running improved style transfer...\")\n",
        "        print(f\"   Style weight: {style_weight}, Content weight: {content_weight}\")\n",
        "        print(f\"   Steps: {num_steps}, Learning rate: {lr}\")\n",
        "\n",
        "        # Load and preprocess images\n",
        "        content_tensor = self.image_loader(content_img)\n",
        "        style_tensor = self.image_loader(style_img)\n",
        "\n",
        "        # Initialize with content image for better results\n",
        "        input_img = content_tensor.clone()\n",
        "\n",
        "        # Set target features for losses by running forward pass\n",
        "        with torch.no_grad():\n",
        "            # Get style targets\n",
        "            self.model(style_tensor)\n",
        "            for i, sl in enumerate(self.style_losses):\n",
        "                # Get the feature from the current forward pass\n",
        "                style_features = []\n",
        "                x = style_tensor\n",
        "                for layer in self.model:\n",
        "                    x = layer(x)\n",
        "                    if isinstance(layer, StyleLoss):\n",
        "                        style_features.append(x)\n",
        "\n",
        "                if i < len(style_features):\n",
        "                    sl.set_target(style_features[i])\n",
        "\n",
        "            # Get content targets\n",
        "            self.model(content_tensor)\n",
        "            for i, cl in enumerate(self.content_losses):\n",
        "                # Get the feature from the current forward pass\n",
        "                content_features = []\n",
        "                x = content_tensor\n",
        "                for layer in self.model:\n",
        "                    x = layer(x)\n",
        "                    if isinstance(layer, ContentLoss):\n",
        "                        content_features.append(x)\n",
        "\n",
        "                if i < len(content_features):\n",
        "                    cl.set_target(content_features[i])\n",
        "\n",
        "        # Optimize the input image\n",
        "        input_img.requires_grad_(True)\n",
        "        optimizer = optim.Adam([input_img], lr=lr)\n",
        "\n",
        "        print(\"   Starting optimization...\")\n",
        "        for step in range(num_steps):\n",
        "            def closure():\n",
        "                # Clamp values to valid range\n",
        "                with torch.no_grad():\n",
        "                    input_img.clamp_(0, 1)\n",
        "\n",
        "                optimizer.zero_grad()\n",
        "                self.model(input_img)\n",
        "\n",
        "                # Calculate losses safely\n",
        "                style_score = 0\n",
        "                for sl in self.style_losses:\n",
        "                    if sl.loss is not None:\n",
        "                        style_score += sl.loss\n",
        "\n",
        "                content_score = 0\n",
        "                for cl in self.content_losses:\n",
        "                    if cl.loss is not None:\n",
        "                        content_score += cl.loss\n",
        "\n",
        "                total_loss = style_weight * style_score + content_weight * content_score\n",
        "                total_loss.backward()\n",
        "\n",
        "                return total_loss\n",
        "\n",
        "            loss = optimizer.step(closure)\n",
        "\n",
        "            if step % 30 == 0:\n",
        "                print(f\"   Step [{step:3d}/{num_steps}] | Loss: {loss:.4f}\")\n",
        "\n",
        "        # Final correction\n",
        "        with torch.no_grad():\n",
        "            input_img.clamp_(0, 1)\n",
        "\n",
        "        print(\"   ✅ Style transfer completed!\")\n",
        "        return input_img\n",
        "\n",
        "def get_better_style_image():\n",
        "    \"\"\"Get a better style image that works well with various content\"\"\"\n",
        "    style_urls = {\n",
        "        'starry_night': 'https://upload.wikimedia.org/wikipedia/commons/thumb/e/ea/Van_Gogh_-_Starry_Night_-_Google_Art_Project.jpg/757px-Van_Gogh_-_Starry_Night_-_Google_Art_Project.jpg',\n",
        "        'great_wave': 'https://upload.wikimedia.org/wikipedia/commons/0/0a/The_Great_Wave_off_Kanagawa.jpg',\n",
        "        'scream': 'https://upload.wikimedia.org/wikipedia/commons/c/c5/Edvard_Munch%2C_1893%2C_The_Scream%2C_oil%2C_tempera_and_pastel_on_cardboard%2C_91_x_73_cm%2C_National_Gallery_of_Norway.jpg'\n",
        "    }\n",
        "\n",
        "    # Try to get Van Gogh's Starry Night (works well with most content)\n",
        "    for style_name, url in style_urls.items():\n",
        "        try:\n",
        "            print(f\"   Downloading {style_name} style...\")\n",
        "            response = requests.get(url, timeout=15)\n",
        "            if response.status_code == 200 and \"image\" in response.headers.get(\"Content-Type\", \"\"):\n",
        "                return Image.open(BytesIO(response.content)).convert('RGB')\n",
        "        except Exception as e:\n",
        "            print(f\"   Failed to get {style_name}: {e}\")\n",
        "            continue\n",
        "\n",
        "    print(\"   Using fallback artistic pattern...\")\n",
        "    return create_artistic_pattern()\n",
        "\n",
        "def create_artistic_pattern():\n",
        "    \"\"\"Create a simple artistic pattern as fallback style\"\"\"\n",
        "    size = 512\n",
        "    image = np.zeros((size, size, 3), dtype=np.uint8)\n",
        "\n",
        "    # Create swirling pattern\n",
        "    for i in range(size):\n",
        "        for j in range(size):\n",
        "            # Create swirling colors\n",
        "            angle = np.arctan2(i - size//2, j - size//2)\n",
        "            radius = np.sqrt((i - size//2)**2 + (j - size//2)**2)\n",
        "\n",
        "            r = int(128 + 127 * np.sin(angle * 3 + radius * 0.01))\n",
        "            g = int(128 + 127 * np.sin(angle * 3 + radius * 0.01 + 2))\n",
        "            b = int(128 + 127 * np.sin(angle * 3 + radius * 0.01 + 4))\n",
        "\n",
        "            image[i, j] = [r, g, b]\n",
        "\n",
        "    return Image.fromarray(image)\n",
        "\n",
        "def neural_style_transfer_improved(content_img, style_img=None, device=\"cuda\"):\n",
        "    \"\"\"Wrapper function for improved neural style transfer\"\"\"\n",
        "\n",
        "    if style_img is None:\n",
        "        print(\"🎨 Getting artistic style image...\")\n",
        "        style_img = get_better_style_image()\n",
        "\n",
        "    # Initialize style transfer\n",
        "    try:\n",
        "        style_transfer = ImprovedStyleTransfer(device)\n",
        "\n",
        "        # Run transfer with optimized parameters\n",
        "        result = style_transfer.run_style_transfer(\n",
        "            content_img,\n",
        "            style_img,\n",
        "            num_steps=150,      # Balanced number of steps\n",
        "            style_weight=50,    # Reduced style weight for better content preservation\n",
        "            content_weight=1,   # Keep content weight at 1\n",
        "            lr=0.01            # Good learning rate\n",
        "        )\n",
        "\n",
        "        # Convert back to PIL Image\n",
        "        unloader = transforms.ToPILImage()\n",
        "        result_img = result.cpu().clone().squeeze(0)\n",
        "        return unloader(result_img)\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"   ❌ Style transfer initialization failed: {e}\")\n",
        "        print(\"   💡 Trying simplified approach...\")\n",
        "\n",
        "        # Fallback: simple style transfer\n",
        "        return simple_style_transfer(content_img, style_img, device)\n",
        "\n",
        "def simple_style_transfer(content_img, style_img, device):\n",
        "    \"\"\"Simplified style transfer as fallback\"\"\"\n",
        "    try:\n",
        "        # Load VGG19\n",
        "        vgg = models.vgg19(weights=models.VGG19_Weights.DEFAULT).features[:21].to(device).eval()\n",
        "\n",
        "        # Image preprocessing\n",
        "        def preprocess_image(image):\n",
        "            if isinstance(image, str):\n",
        "                image = Image.open(image)\n",
        "            if image.mode != 'RGB':\n",
        "                image = image.convert('RGB')\n",
        "\n",
        "            transform = transforms.Compose([\n",
        "                transforms.Resize((512, 512)),\n",
        "                transforms.ToTensor()\n",
        "            ])\n",
        "            return transform(image).unsqueeze(0).to(device)\n",
        "\n",
        "        def gram_matrix(tensor):\n",
        "            b, c, h, w = tensor.size()\n",
        "            features = tensor.view(c, h * w)\n",
        "            G = torch.mm(features, features.t())\n",
        "            return G.div(c * h * w)\n",
        "\n",
        "        # Preprocess images\n",
        "        content = preprocess_image(content_img)\n",
        "        style = preprocess_image(style_img)\n",
        "        generated = content.clone().requires_grad_(True)\n",
        "\n",
        "        optimizer = torch.optim.Adam([generated], lr=0.01)\n",
        "\n",
        "        print(\"   Running simplified style transfer...\")\n",
        "        for step in range(100):  # Reduced steps\n",
        "            # Extract features\n",
        "            content_features = vgg(content)\n",
        "            style_features = vgg(style)\n",
        "            gen_features = vgg(generated)\n",
        "\n",
        "            # Content loss (using last layer)\n",
        "            content_loss = torch.mean((gen_features - content_features) ** 2)\n",
        "\n",
        "            # Style loss (using Gram matrices)\n",
        "            gen_gram = gram_matrix(gen_features)\n",
        "            style_gram = gram_matrix(style_features)\n",
        "            style_loss = torch.mean((gen_gram - style_gram) ** 2)\n",
        "\n",
        "            # Total loss\n",
        "            total_loss = content_loss + 100 * style_loss  # Lower style weight\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            total_loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            # Clamp values\n",
        "            with torch.no_grad():\n",
        "                generated.clamp_(0, 1)\n",
        "\n",
        "            if step % 20 == 0:\n",
        "                print(f\"   Step [{step:3d}/100] | Loss: {total_loss.item():.4f}\")\n",
        "\n",
        "        # Convert to PIL\n",
        "        result_tensor = generated.clone().detach().cpu().squeeze(0)\n",
        "        unloader = transforms.ToPILImage()\n",
        "        return unloader(result_tensor)\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"   ❌ Even simplified style transfer failed: {e}\")\n",
        "        print(\"   📷 Returning original image...\")\n",
        "        return content_img\n",
        "\n",
        "# -------------------------\n",
        "# 4. MAIN EXECUTION\n",
        "# -------------------------\n",
        "if __name__ == \"__main__\":\n",
        "    print(\"=\" * 60)\n",
        "    print(\"🎨 AI ART GENERATOR - Final Year Project\")\n",
        "    print(\"   Prompt Enhancement + Stable Diffusion + Neural Style Transfer\")\n",
        "    print(\"=\" * 60)\n",
        "\n",
        "    # User input\n",
        "    user_prompt = input(\"\\n💭 Enter your art prompt: \")\n",
        "\n",
        "    if not user_prompt.strip():\n",
        "        print(\"❌ Please enter a valid prompt!\")\n",
        "        exit()\n",
        "\n",
        "    print(f\"\\n📝 Original prompt: '{user_prompt}'\")\n",
        "\n",
        "    # Step 1: Enhance prompt\n",
        "    print(\"\\n\" + \"─\" * 50)\n",
        "    print(\"STEP 1: ENHANCING PROMPT\")\n",
        "    print(\"─\" * 50)\n",
        "    enhanced_prompt = enhance_prompt(user_prompt)\n",
        "    print(f\"✨ Enhanced prompt: '{enhanced_prompt}'\")\n",
        "\n",
        "    # Step 2: Generate base image\n",
        "    print(\"\\n\" + \"─\" * 50)\n",
        "    print(\"STEP 2: GENERATING BASE IMAGE\")\n",
        "    print(\"─\" * 50)\n",
        "    base_img = generate_image(enhanced_prompt, filename=\"generated.png\")\n",
        "\n",
        "    if base_img is None:\n",
        "        print(\"❌ Failed to generate image. Exiting.\")\n",
        "        exit()\n",
        "\n",
        "    # Step 3: Apply Neural Style Transfer\n",
        "    print(\"\\n\" + \"─\" * 50)\n",
        "    print(\"STEP 3: APPLYING NEURAL STYLE TRANSFER\")\n",
        "    print(\"─\" * 50)\n",
        "\n",
        "    try:\n",
        "        styled_img = neural_style_transfer_improved(base_img, device=device)\n",
        "        styled_img.save(\"styled_final.png\")\n",
        "        print(\"✅ Final styled image saved as 'styled_final.png'\")\n",
        "\n",
        "        # Save comparison\n",
        "        print(\"\\n📊 Creating comparison image...\")\n",
        "        comparison = Image.new('RGB', (base_img.width * 2, base_img.height))\n",
        "        comparison.paste(base_img, (0, 0))\n",
        "        comparison.paste(styled_img, (base_img.width, 0))\n",
        "        comparison.save(\"comparison.png\")\n",
        "        print(\"✅ Comparison saved as 'comparison.png'\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"❌ Style transfer failed: {e}\")\n",
        "        print(\"💡 You can still use the generated image: 'generated.png'\")\n",
        "\n",
        "    print(\"\\n\" + \"=\" * 60)\n",
        "    print(\"🎉 AI ART GENERATION COMPLETE!\")\n",
        "    print(\"Generated files:\")\n",
        "    print(\"   📄 generated.png - Original Stable Diffusion output\")\n",
        "    if os.path.exists(\"styled_final.png\"):\n",
        "        print(\"   🎨 styled_final.png - Neural style transfer result\")\n",
        "    if os.path.exists(\"comparison.png\"):\n",
        "        print(\"   📊 comparison.png - Side-by-side comparison\")\n",
        "    print(\"=\" * 60)"
      ],
      "metadata": {
        "id": "xaU3w_ak1_Ka"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}