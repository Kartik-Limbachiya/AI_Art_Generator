# -*- coding: utf-8 -*-
"""Ai_Art_Generator_Project.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1hthlM1Fbm1WIVuWUXUBLMh_5sikbUnDO
"""

!pip install diffusers transformers accelerate safetensors torch torchvision gradio

import torch
import torch.nn as nn
import torch.optim as optim
from PIL import Image
import torchvision.transforms as transforms
import torchvision.models as models
import copy

# Load and preprocess image
def image_loader(image_path, imsize=512):
    loader = transforms.Compose([
        transforms.Resize((imsize, imsize)),
        transforms.ToTensor()
    ])
    image = Image.open(image_path)
    image = loader(image).unsqueeze(0)
    return image.to(torch.device("cuda" if torch.cuda.is_available() else "cpu"), torch.float)

# Model
cnn = models.vgg19(weights=models.VGG19_Weights.DEFAULT).features.eval().to("cuda")

# Style transfer function (simplified)
def run_style_transfer(cnn, content_img, style_img, num_steps=200):
    input_img = content_img.clone()
    optimizer = optim.LBFGS([input_img.requires_grad_()])
    mse_loss = nn.MSELoss()

    style_weight = 1e6
    content_weight = 1

    for step in range(num_steps):
        def closure():
            optimizer.zero_grad()
            # simple content loss (for demo, you can expand later)
            loss = mse_loss(input_img, content_img) + style_weight * mse_loss(input_img, style_img)
            loss.backward()
            return loss
        optimizer.step(closure)

    return input_img.detach()

from diffusers import StableDiffusionPipeline

pipe = StableDiffusionPipeline.from_pretrained(
    "runwayml/stable-diffusion-v1-5",
    torch_dtype=torch.float16
)
pipe = pipe.to("cuda")

def generate_image(prompt):
    image = pipe(prompt).images[0]
    return image

import gradio as gr

def style_transfer_ui(content, style):
    content_img = image_loader(content.name)
    style_img = image_loader(style.name)
    output = run_style_transfer(cnn, content_img, style_img)
    return transforms.ToPILImage()(output.squeeze(0))

def text2img_ui(prompt):
    return generate_image(prompt)

with gr.Blocks() as demo:
    gr.Markdown("# üé® AI Art Generator")

    with gr.Tab("Text-to-Image (Stable Diffusion)"):
        prompt_input = gr.Textbox(label="Enter Prompt")
        output_img2 = gr.Image()
        btn2 = gr.Button("Generate AI Image")
        btn2.click(fn=text2img_ui, inputs=prompt_input, outputs=output_img2)

demo.launch(share=True)

import torch
import torch.nn as nn
import torch.optim as optim
import torchvision.models as models
import torchvision.transforms as transforms
from PIL import Image
import gradio as gr

# ===================== #
# Image Preprocessing
# ===================== #
imsize = 512 if torch.cuda.is_available() else 256

loader = transforms.Compose([
    transforms.Resize((imsize, imsize)),
    transforms.ToTensor()
])

def image_loader(image_path):
    image = Image.open(image_path).convert("RGB")
    image = loader(image).unsqueeze(0)  # [1,3,H,W]
    return image.to(device, torch.float)

unloader = transforms.ToPILImage()

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

# ===================== #
# Load VGG19 Model
# ===================== #
cnn = models.vgg19(weights=models.VGG19_Weights.DEFAULT).features.to(device).eval()

# Content & Style Layers
content_layers = ['conv_4']
style_layers = ['conv_1', 'conv_2', 'conv_3', 'conv_4', 'conv_5']

# ===================== #
# Loss Functions
# ===================== #
class ContentLoss(nn.Module):
    def __init__(self, target):
        super(ContentLoss, self).__init__()
        self.target = target.detach()

    def forward(self, x):
        self.loss = nn.functional.mse_loss(x, self.target)
        return x

def gram_matrix(x):
    a, b, c, d = x.size()
    features = x.view(a * b, c * d)
    G = torch.mm(features, features.t())
    return G.div(a * b * c * d)

class StyleLoss(nn.Module):
    def __init__(self, target_feature):
        super(StyleLoss, self).__init__()
        self.target = gram_matrix(target_feature).detach()

    def forward(self, x):
        G = gram_matrix(x)
        self.loss = nn.functional.mse_loss(G, self.target)
        return x

# ===================== #
# Style Transfer Function
# ===================== #
def run_style_transfer(content_path, style_path, num_steps=200,
                       style_weight=1e6, content_weight=1):
    # Load images
    content_img = image_loader(content_path)
    style_img = image_loader(style_path)

    assert content_img.size() == style_img.size(), \
        "Content and Style images must be the same size"

    input_img = content_img.clone()

    # Build model
    cnn_normalization_mean = torch.tensor([0.485, 0.456, 0.406]).to(device)
    cnn_normalization_std = torch.tensor([0.229, 0.224, 0.225]).to(device)

    normalization = transforms.Normalize(mean=cnn_normalization_mean,
                                         std=cnn_normalization_std)

    content_losses = []
    style_losses = []

    model = nn.Sequential(normalization)

    i = 0
    for layer in cnn.children():
        if isinstance(layer, nn.Conv2d):
            i += 1
            name = f'conv_{i}'
        elif isinstance(layer, nn.ReLU):
            name = f'relu_{i}'
            layer = nn.ReLU(inplace=False)
        elif isinstance(layer, nn.MaxPool2d):
            name = f'pool_{i}'
        elif isinstance(layer, nn.BatchNorm2d):
            name = f'bn_{i}'
        else:
            raise RuntimeError(f"Unrecognized layer: {layer.__class__.__name__}")

        model.add_module(name, layer)

        if name in content_layers:
            target = model(content_img).detach()
            content_loss = ContentLoss(target)
            model.add_module(f"content_loss_{i}", content_loss)
            content_losses.append(content_loss)

        if name in style_layers:
            target_feature = model(style_img).detach()
            style_loss = StyleLoss(target_feature)
            model.add_module(f"style_loss_{i}", style_loss)
            style_losses.append(style_loss)

    # Trim model after last loss
    for i in range(len(model) - 1, -1, -1):
        if isinstance(model[i], ContentLoss) or isinstance(model[i], StyleLoss):
            break
    model = model[:i+1]

    optimizer = optim.LBFGS([input_img.requires_grad_()])

    run = [0]
    while run[0] <= num_steps:
        def closure():
            input_img.data.clamp_(0, 1)

            optimizer.zero_grad()
            model(input_img)
            style_score = sum([sl.loss for sl in style_losses])
            content_score = sum([cl.loss for cl in content_losses])

            loss = style_score * style_weight + content_score * content_weight
            loss.backward()

            run[0] += 1
            return loss
        optimizer.step(closure)

    input_img.data.clamp_(0, 1)

    # Convert tensor to image
    output = input_img.cpu().clone()
    output = output.squeeze(0)
    image = unloader(output)
    return image

# ===================== #
# Gradio UI
# ===================== #
def transfer(content, style):
    output = run_style_transfer(content, style, num_steps=200)
    return output

with gr.Blocks() as demo:
    gr.Markdown("üé® **Neural Style Transfer**")
    with gr.Row():
        content = gr.Image(type="filepath", label="Upload Content Image")
        style = gr.Image(type="filepath", label="Upload Style Image")
    output = gr.Image(label="Styled Output")

    btn = gr.Button("Transfer Style")
    btn.click(transfer, inputs=[content, style], outputs=[output])

demo.launch()

# ==========================================
# Final Year Major Project ‚Äì BSC.IT
# AI Prompt Enhancer + Stable Diffusion + Improved Neural Style Transfer
# ==========================================

import requests
import torch
from io import BytesIO
from PIL import Image
from diffusers import DiffusionPipeline
from torchvision import transforms, models
import torch.nn as nn
import torch.optim as optim
import os
import numpy as np

# -------------------------
# CONFIGURATION
# -------------------------
GEMINI_API_KEY = os.getenv("GEMINI_API_KEY", "AIzaSyC38sUGvasn8QDBoMAvzkHt7H0jrRdCoGo")  # Use env variable
HF_MODEL = "runwayml/stable-diffusion-v1-5"

device = "cuda" if torch.cuda.is_available() else "cpu"
print(f"‚úÖ Using device: {device}")

# -------------------------
# 1. AI Prompt Enhancer (Gemini API)
# -------------------------
def enhance_prompt(prompt: str) -> str:
    """Enhance user prompt using Gemini API for better Stable Diffusion results"""
    url = "https://generativelanguage.googleapis.com/v1beta/models/gemini-2.0-flash:generateContent"
    headers = {"Content-Type": "application/json"}
    payload = {
        "contents": [{"parts": [{"text": f"Enhance this prompt for Stable Diffusion art generation. Make it more detailed and artistic but keep it under 70 words. Return only the enhanced prompt, no explanations:\nOriginal prompt: {prompt}"}]}]
    }

    try:
        response = requests.post(f"{url}?key={GEMINI_API_KEY}", headers=headers, json=payload, timeout=10)
        data = response.json()

        enhanced = data["candidates"][0]["content"]["parts"][0]["text"]
        # Clean out markdown formatting or explanations
        enhanced = enhanced.strip().split("```")[0].strip('"').strip()

        # Additional check: if still too long, truncate intelligently
        words = enhanced.split()
        if len(words) > 70:
            enhanced = ' '.join(words[:70])

        print(f"‚ú® Prompt enhanced successfully!")
        return enhanced
    except Exception as e:
        print(f"‚ö†Ô∏è Gemini enhancement failed: {e}. Using original prompt.")
        return prompt

# -------------------------
# 2. AI Image Generator (Stable Diffusion)
# -------------------------
def generate_image(prompt: str, filename="generated.png"):
    """Generate image using Stable Diffusion"""
    print("üé® Generating image with Stable Diffusion...")
    try:
        pipe = DiffusionPipeline.from_pretrained(HF_MODEL, torch_dtype=torch.float16)
        pipe = pipe.to(device)

        # Generate image with improved settings
        image = pipe(
            prompt,
            num_inference_steps=50,
            guidance_scale=7.5,
            width=512,
            height=512
        ).images[0]

        image.save(filename)
        print(f"‚úÖ Image saved as {filename}")
        return image
    except Exception as e:
        print(f"‚ùå Image generation failed: {e}")
        return None

# -------------------------
# 3. Improved Neural Style Transfer
# -------------------------

class ContentLoss(nn.Module):
    """Content loss for preserving image content"""
    def __init__(self):
        super(ContentLoss, self).__init__()
        self.target = None
        self.loss = None

    def set_target(self, target):
        self.target = target.detach()

    def forward(self, x):
        if self.target is not None:
            self.loss = nn.functional.mse_loss(x, self.target)
        return x

class StyleLoss(nn.Module):
    """Style loss using Gram matrices"""
    def __init__(self):
        super(StyleLoss, self).__init__()
        self.target = None
        self.loss = None

    def gram_matrix(self, tensor):
        b, c, h, w = tensor.size()
        features = tensor.view(b * c, h * w)
        G = torch.mm(features, features.t())
        return G.div(b * c * h * w)

    def set_target(self, target):
        self.target = self.gram_matrix(target).detach()

    def forward(self, x):
        if self.target is not None:
            G = self.gram_matrix(x)
            self.loss = nn.functional.mse_loss(G, self.target)
        return x

class ImprovedStyleTransfer:
    """Improved Neural Style Transfer using VGG19"""
    def __init__(self, device):
        self.device = device

        # Load pre-trained VGG19
        vgg = models.vgg19(weights=models.VGG19_Weights.DEFAULT).features.to(device).eval()

        # Normalization for VGG19
        self.normalization = transforms.Normalize(
            mean=[0.485, 0.456, 0.406],
            std=[0.229, 0.224, 0.225]
        ).to(device)

        # Define which layers to use for content and style
        self.content_layers = ['conv_4']
        self.style_layers = ['conv_1', 'conv_2', 'conv_3', 'conv_4', 'conv_5']

        self.model, self.style_losses, self.content_losses = self.build_model(vgg)

    def build_model(self, cnn):
        """Build the neural network with loss functions"""
        content_losses = []
        style_losses = []

        model = nn.Sequential(self.normalization)

        i = 0
        for layer in cnn.children():
            if isinstance(layer, nn.Conv2d):
                i += 1
                name = f'conv_{i}'
            elif isinstance(layer, nn.ReLU):
                name = f'relu_{i}'
                layer = nn.ReLU(inplace=False)  # Important: don't use inplace
            elif isinstance(layer, nn.MaxPool2d):
                name = f'pool_{i}'
            elif isinstance(layer, nn.BatchNorm2d):
                name = f'bn_{i}'
            else:
                raise RuntimeError(f'Unrecognized layer: {layer.__class__.__name__}')

            model.add_module(name, layer)

            if name in self.content_layers:
                content_loss = ContentLoss()
                model.add_module(f"content_loss_{i}", content_loss)
                content_losses.append(content_loss)

            if name in self.style_layers:
                style_loss = StyleLoss()
                model.add_module(f"style_loss_{i}", style_loss)
                style_losses.append(style_loss)

        # Trim model after last loss layer
        for i in range(len(model) - 1, -1, -1):
            if isinstance(model[i], (ContentLoss, StyleLoss)):
                break

        model = model[:(i + 1)]
        return model, style_losses, content_losses

    def image_loader(self, image, imsize=512):
        """Load and preprocess image"""
        if isinstance(image, str):
            image = Image.open(image)

        if image.mode != 'RGB':
            image = image.convert('RGB')

        loader = transforms.Compose([
            transforms.Resize((imsize, imsize)),
            transforms.ToTensor()
        ])

        image = loader(image).unsqueeze(0)
        return image.to(self.device, torch.float)

    def run_style_transfer(self, content_img, style_img, num_steps=150,
                          style_weight=100, content_weight=1, lr=0.01):
        """Main style transfer function with optimized parameters"""

        print(f"üé≠ Running improved style transfer...")
        print(f"   Style weight: {style_weight}, Content weight: {content_weight}")
        print(f"   Steps: {num_steps}, Learning rate: {lr}")

        # Load and preprocess images
        content_tensor = self.image_loader(content_img)
        style_tensor = self.image_loader(style_img)

        # Initialize with content image for better results
        input_img = content_tensor.clone()

        # Set target features for losses by running forward pass
        with torch.no_grad():
            # Get style targets
            self.model(style_tensor)
            for i, sl in enumerate(self.style_losses):
                # Get the feature from the current forward pass
                style_features = []
                x = style_tensor
                for layer in self.model:
                    x = layer(x)
                    if isinstance(layer, StyleLoss):
                        style_features.append(x)

                if i < len(style_features):
                    sl.set_target(style_features[i])

            # Get content targets
            self.model(content_tensor)
            for i, cl in enumerate(self.content_losses):
                # Get the feature from the current forward pass
                content_features = []
                x = content_tensor
                for layer in self.model:
                    x = layer(x)
                    if isinstance(layer, ContentLoss):
                        content_features.append(x)

                if i < len(content_features):
                    cl.set_target(content_features[i])

        # Optimize the input image
        input_img.requires_grad_(True)
        optimizer = optim.Adam([input_img], lr=lr)

        print("   Starting optimization...")
        for step in range(num_steps):
            def closure():
                # Clamp values to valid range
                with torch.no_grad():
                    input_img.clamp_(0, 1)

                optimizer.zero_grad()
                self.model(input_img)

                # Calculate losses safely
                style_score = 0
                for sl in self.style_losses:
                    if sl.loss is not None:
                        style_score += sl.loss

                content_score = 0
                for cl in self.content_losses:
                    if cl.loss is not None:
                        content_score += cl.loss

                total_loss = style_weight * style_score + content_weight * content_score
                total_loss.backward()

                return total_loss

            loss = optimizer.step(closure)

            if step % 30 == 0:
                print(f"   Step [{step:3d}/{num_steps}] | Loss: {loss:.4f}")

        # Final correction
        with torch.no_grad():
            input_img.clamp_(0, 1)

        print("   ‚úÖ Style transfer completed!")
        return input_img

def get_better_style_image():
    """Get a better style image that works well with various content"""
    style_urls = {
        'starry_night': 'https://upload.wikimedia.org/wikipedia/commons/thumb/e/ea/Van_Gogh_-_Starry_Night_-_Google_Art_Project.jpg/757px-Van_Gogh_-_Starry_Night_-_Google_Art_Project.jpg',
        'great_wave': 'https://upload.wikimedia.org/wikipedia/commons/0/0a/The_Great_Wave_off_Kanagawa.jpg',
        'scream': 'https://upload.wikimedia.org/wikipedia/commons/c/c5/Edvard_Munch%2C_1893%2C_The_Scream%2C_oil%2C_tempera_and_pastel_on_cardboard%2C_91_x_73_cm%2C_National_Gallery_of_Norway.jpg'
    }

    # Try to get Van Gogh's Starry Night (works well with most content)
    for style_name, url in style_urls.items():
        try:
            print(f"   Downloading {style_name} style...")
            response = requests.get(url, timeout=15)
            if response.status_code == 200 and "image" in response.headers.get("Content-Type", ""):
                return Image.open(BytesIO(response.content)).convert('RGB')
        except Exception as e:
            print(f"   Failed to get {style_name}: {e}")
            continue

    print("   Using fallback artistic pattern...")
    return create_artistic_pattern()

def create_artistic_pattern():
    """Create a simple artistic pattern as fallback style"""
    size = 512
    image = np.zeros((size, size, 3), dtype=np.uint8)

    # Create swirling pattern
    for i in range(size):
        for j in range(size):
            # Create swirling colors
            angle = np.arctan2(i - size//2, j - size//2)
            radius = np.sqrt((i - size//2)**2 + (j - size//2)**2)

            r = int(128 + 127 * np.sin(angle * 3 + radius * 0.01))
            g = int(128 + 127 * np.sin(angle * 3 + radius * 0.01 + 2))
            b = int(128 + 127 * np.sin(angle * 3 + radius * 0.01 + 4))

            image[i, j] = [r, g, b]

    return Image.fromarray(image)

def neural_style_transfer_improved(content_img, style_img=None, device="cuda"):
    """Wrapper function for improved neural style transfer"""

    if style_img is None:
        print("üé® Getting artistic style image...")
        style_img = get_better_style_image()

    # Initialize style transfer
    try:
        style_transfer = ImprovedStyleTransfer(device)

        # Run transfer with optimized parameters
        result = style_transfer.run_style_transfer(
            content_img,
            style_img,
            num_steps=150,      # Balanced number of steps
            style_weight=50,    # Reduced style weight for better content preservation
            content_weight=1,   # Keep content weight at 1
            lr=0.01            # Good learning rate
        )

        # Convert back to PIL Image
        unloader = transforms.ToPILImage()
        result_img = result.cpu().clone().squeeze(0)
        return unloader(result_img)

    except Exception as e:
        print(f"   ‚ùå Style transfer initialization failed: {e}")
        print("   üí° Trying simplified approach...")

        # Fallback: simple style transfer
        return simple_style_transfer(content_img, style_img, device)

def simple_style_transfer(content_img, style_img, device):
    """Simplified style transfer as fallback"""
    try:
        # Load VGG19
        vgg = models.vgg19(weights=models.VGG19_Weights.DEFAULT).features[:21].to(device).eval()

        # Image preprocessing
        def preprocess_image(image):
            if isinstance(image, str):
                image = Image.open(image)
            if image.mode != 'RGB':
                image = image.convert('RGB')

            transform = transforms.Compose([
                transforms.Resize((512, 512)),
                transforms.ToTensor()
            ])
            return transform(image).unsqueeze(0).to(device)

        def gram_matrix(tensor):
            b, c, h, w = tensor.size()
            features = tensor.view(c, h * w)
            G = torch.mm(features, features.t())
            return G.div(c * h * w)

        # Preprocess images
        content = preprocess_image(content_img)
        style = preprocess_image(style_img)
        generated = content.clone().requires_grad_(True)

        optimizer = torch.optim.Adam([generated], lr=0.01)

        print("   Running simplified style transfer...")
        for step in range(100):  # Reduced steps
            # Extract features
            content_features = vgg(content)
            style_features = vgg(style)
            gen_features = vgg(generated)

            # Content loss (using last layer)
            content_loss = torch.mean((gen_features - content_features) ** 2)

            # Style loss (using Gram matrices)
            gen_gram = gram_matrix(gen_features)
            style_gram = gram_matrix(style_features)
            style_loss = torch.mean((gen_gram - style_gram) ** 2)

            # Total loss
            total_loss = content_loss + 100 * style_loss  # Lower style weight

            optimizer.zero_grad()
            total_loss.backward()
            optimizer.step()

            # Clamp values
            with torch.no_grad():
                generated.clamp_(0, 1)

            if step % 20 == 0:
                print(f"   Step [{step:3d}/100] | Loss: {total_loss.item():.4f}")

        # Convert to PIL
        result_tensor = generated.clone().detach().cpu().squeeze(0)
        unloader = transforms.ToPILImage()
        return unloader(result_tensor)

    except Exception as e:
        print(f"   ‚ùå Even simplified style transfer failed: {e}")
        print("   üì∑ Returning original image...")
        return content_img

# -------------------------
# 4. MAIN EXECUTION
# -------------------------
if __name__ == "__main__":
    print("=" * 60)
    print("üé® AI ART GENERATOR - Final Year Project")
    print("   Prompt Enhancement + Stable Diffusion + Neural Style Transfer")
    print("=" * 60)

    # User input
    user_prompt = input("\nüí≠ Enter your art prompt: ")

    if not user_prompt.strip():
        print("‚ùå Please enter a valid prompt!")
        exit()

    print(f"\nüìù Original prompt: '{user_prompt}'")

    # Step 1: Enhance prompt
    print("\n" + "‚îÄ" * 50)
    print("STEP 1: ENHANCING PROMPT")
    print("‚îÄ" * 50)
    enhanced_prompt = enhance_prompt(user_prompt)
    print(f"‚ú® Enhanced prompt: '{enhanced_prompt}'")

    # Step 2: Generate base image
    print("\n" + "‚îÄ" * 50)
    print("STEP 2: GENERATING BASE IMAGE")
    print("‚îÄ" * 50)
    base_img = generate_image(enhanced_prompt, filename="generated.png")

    if base_img is None:
        print("‚ùå Failed to generate image. Exiting.")
        exit()

    # Step 3: Apply Neural Style Transfer
    print("\n" + "‚îÄ" * 50)
    print("STEP 3: APPLYING NEURAL STYLE TRANSFER")
    print("‚îÄ" * 50)

    try:
        styled_img = neural_style_transfer_improved(base_img, device=device)
        styled_img.save("styled_final.png")
        print("‚úÖ Final styled image saved as 'styled_final.png'")

        # Save comparison
        print("\nüìä Creating comparison image...")
        comparison = Image.new('RGB', (base_img.width * 2, base_img.height))
        comparison.paste(base_img, (0, 0))
        comparison.paste(styled_img, (base_img.width, 0))
        comparison.save("comparison.png")
        print("‚úÖ Comparison saved as 'comparison.png'")

    except Exception as e:
        print(f"‚ùå Style transfer failed: {e}")
        print("üí° You can still use the generated image: 'generated.png'")

    print("\n" + "=" * 60)
    print("üéâ AI ART GENERATION COMPLETE!")
    print("Generated files:")
    print("   üìÑ generated.png - Original Stable Diffusion output")
    if os.path.exists("styled_final.png"):
        print("   üé® styled_final.png - Neural style transfer result")
    if os.path.exists("comparison.png"):
        print("   üìä comparison.png - Side-by-side comparison")
    print("=" * 60)

!git init
!git remote add origin https://github.com/Kartik-Limbachiya/AI_Art_Generator.git

!git add .
!git commit -m "Initial commit: AI Prompt Enhancer + Stable Diffusion + Neural Style Transfer"
!git branch -M main
!git push -u origin main

import os
from google.colab import userdata

# Retrieve the GitHub token from Colab secrets
# Make sure you have added a secret named 'GH_TOKEN' in the Colab secrets panel
github_token = userdata.get('GH_TOKEN')

if github_token:
    # Use the token with the git push command
    # This format includes the token in the URL (use HTTPS)
    repo_url = f"https://{github_token}@github.com/Kartik-Limbachiya/AI_Art_Generator.git"
    print("Attempting to push using token...")
    !git push -u {repo_url} main
else:
    print("GitHub token not found in Colab secrets. Please add it.")
    print("Go to üîë > Add new secret > Name: GH_TOKEN, Value: <Your GitHub Token>")